{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChessML Netz\n",
    "\n",
    "This neural network based on PyTorch will try to classifiy chess pieces from photos taken from the top of a chessboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining all possible classes\n",
    "\n",
    "bb = Black Bishop\n",
    "bk = Black King\n",
    "bn = Black Knight\n",
    "bp = Black Pawn\n",
    "bq = Black Queen\n",
    "br = Black Rook\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\"bb\", \"bk\", \"bn\", \"bp\", \"bq\", \"br\", \"wb\", \"wk\", \"wn\", \"wp\", \"wq\", \"wr\", \"empty\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a normalization function for the analyzed data\n",
    "\n",
    "The values are recommended by a PyTorch tutorial (https://youtu.be/32lHVbT09h8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(128),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChessNet, self).__init__()\n",
    "        # Defining the convolutional layers of the net\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(6, 12, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(12, 24, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(24, 48, kernel_size=3)\n",
    "        self.conv5 = nn.Conv2d(48, 96, kernel_size=3)\n",
    "\n",
    "        # Defining dropout layer to prevent from memorizing\n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "        self.dropout2 = nn.Dropout(p=0.3)\n",
    "\n",
    "        # Defining the fully connected layers of the net\n",
    "        self.fc1 = nn.Linear(384, 64)  # 96*2*2 = 384\n",
    "        self.fc2 = nn.Linear(64, 13)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)  # Relu because it's famous\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = x.view(-1, 384)  # Convert 2d data to 1d\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return torch.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, train_data, optimizer, criterion):\n",
    "    model.train()    \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i in range(len(train_data)):\n",
    "        data, target = train_data[i][0], train_data[i][1]\n",
    "        target = torch.Tensor(target)\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(data)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, validation_data, criterion):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for i in range(len(validation_data)):\n",
    "        data, target = validation_data[i][0], validation_data[i][1]\n",
    "        target = torch.Tensor(target)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "            \n",
    "        out = model(data)\n",
    "\n",
    "        loss += criterion(out, target).item()\n",
    "        \n",
    "        _, prediction = torch.max(out.data, 1)\n",
    "        correct += (prediction == target.data).sum().item()\n",
    "\n",
    "    loss = loss / len(validation_data)\n",
    "    print(\"###################################\")\n",
    "    print(\"Average loss:\", loss)\n",
    "    print(\"Accuracy:\", 100. * correct / len(validation_data))\n",
    "    print(\"###################################\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "The test function is not really necessary. It is just for a better understanding and eventually testing the neural network manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, path):\n",
    "    model.eval()\n",
    "    files = os.listdir(path)\n",
    "    file = random.choice(files)\n",
    "    img = Image.open(os.path.join(path, file))\n",
    "    img_eval_tensor = transform(img)\n",
    "    img_eval_tensor.unsqueeze_(0)\n",
    "    if torch.cuda.is_available():\n",
    "        img_eval_tensor = img_eval_tensor.cuda()\n",
    "    out = model(img_eval_tensor)\n",
    "    print(torch.max(out))\n",
    "    print(torch.max(out, 1)[0], torch.max(out, 1)[1])\n",
    "    print(torch.max(out, 1, keepdim=True))\n",
    "    print(CLASSES[out.data.max(1, keepdim=True)[1]])\n",
    "    img.show()\n",
    "    input(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "At first all available files are added to a file list. In addition the label of the file will be generated and added to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(path):\n",
    "    file_list = []\n",
    "    for d in os.listdir(path):\n",
    "        if os.path.isdir(os.path.join(path, d)):\n",
    "            for f in os.listdir(os.path.join(path, d)):\n",
    "                file_list.append([os.path.join(path, d, f), hot_encode_label(d)])\n",
    "    return file_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating tensors from the file list, we just created. All files get mixed up randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tensors(file_list, batch_size):\n",
    "    data_list = []\n",
    "    data = []\n",
    "    label_list = []\n",
    "    files_count = len(file_list)\n",
    "    for i in range(files_count):\n",
    "        file = random.choice(file_list)\n",
    "        file_list.remove(file)\n",
    "        \n",
    "        img = Image.open(file[0])  # Index 0: filename, Index 1: label\n",
    "        img_tensor = transform(img)\n",
    "        data_list.append(img_tensor)\n",
    "        label_list.append(file[1])\n",
    "        \n",
    "        if len(data_list) >= batch_size:\n",
    "            data.append((torch.stack(data_list), label_list))\n",
    "            data_list = []\n",
    "            label_list = []\n",
    "            \n",
    "            #Statistics\n",
    "            print(\"Loaded batch\", len(data), \"of\", int(files_count / batch_size))\n",
    "            print(\"Percentage Done:\", len(data) / int(files_count / batch_size) * 100, \"%\")\n",
    "            if len(data) >= 1:\n",
    "                break\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label will be hot encoded, since there is only one possible class for each tile of the chessboard. \n",
    "This means that for example a black knight will be labeled by the vector: \n",
    "\n",
    ">\\[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_encode_label(label):\n",
    "    index = CLASSES.index(label)\n",
    "    vector = np.zeros(len(CLASSES), np.uint8)\n",
    "    vector[index] = 1\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The save model function will save the state of a model after a specific epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch):\n",
    "    torch.save(model.state_dict(), \"model/chess-net_{}.pt\".format(epoch))\n",
    "    print(\"Checkpoint saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main\n",
    "\n",
    "Training and evaluating the ChessNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Loading Training Data\")\n",
    "    train_files = read_files(\"data/train_augmented\")\n",
    "    train_data = generate_tensors(train_files, 32)\n",
    "\n",
    "    print(\"Loading Validation Data\")\n",
    "    validation_files = read_files(\"data/validation_augmented\")\n",
    "    validation_data = generate_tensors(validation_files, 32)\n",
    "\n",
    "    model = ChessNet()\n",
    "    \n",
    "    # Activate cuda support if available\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    # Check if model is already available\n",
    "    # if os.path.isfile(\"model/chess-net.pt\"):\n",
    "    #    model = torch.load(\"model/chess-net.pt\")\n",
    "\n",
    "    # Defining the optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    # optimizer = optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    criterion = F.binary_cross_entropy\n",
    "    \n",
    "    # Start training\n",
    "    start = time.time()\n",
    "    for epoch in range(1,5):\n",
    "        train(model, epoch, train_data, optimizer, criterion)\n",
    "        validate(model, validation_data, criterion)\n",
    "        # test(model, \"data/train_augmented/bn\")\n",
    "        save_model(model, epoch)\n",
    "    end = time.time()\n",
    "    print(\"Training of the neuroal network done.\")\n",
    "    print(\"Time spent:\", end-start, \"s\")\n",
    "    \n",
    "    # Start validation\n",
    "    # start = time.time()\n",
    "    # validate(model, eval_data, criterion)\n",
    "    # end = time.time()\n",
    "    # print(\"Evaluation of the neuroal network done.\")\n",
    "    # print(\"Time:\", end-start, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Training Data\n",
      "Loaded batch 1 of 706\n",
      "Percentage Done: 0.141643059490085 %\n",
      "Loading Validation Data\n",
      "Loaded batch 1 of 139\n",
      "Percentage Done: 0.7194244604316548 %\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (13) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-8a56a73aaea8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;31m# test(model, \"data/train_augmented/bn\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-6b21e2bfb8a6>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, validation_data, criterion)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (13) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
